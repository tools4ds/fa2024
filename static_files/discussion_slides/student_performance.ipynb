{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Week 2 - Introduction to Pandas and Scikit-Learn\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/fa2024/blob/main/static_files/discussion_slides/student_performance.ipynb)\n",
    "\n",
    "- This Jupyter Notebook will brief you with an introduction to Pandas and Scikit-Learn, as we get our hands dirty on a dataset.\n",
    "- We will first take a look at how to view and understand the dataset, followed by some preprocessing, exploratory data analysis and visualizations of various trends in the dataset.\n",
    "- We then will dive into an example of building a Machine Learning model that explores the relationship between various factors specified in the dataset and predict the 'student exam scores'. \n",
    "- The aim is to understand these relationships and predict exam scores using a ML model (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin by first reading the CSV file that contains the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('StudentPerformanceFactors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can then take a look at the shape of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to view the first 5 records of the dataset, we need to use the .head() function, and .tail() to view the last 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To drop null values present in the dataset, we can use the .dropna() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the data types present in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas allows you to reference a column similar to a python dictionary key, using column names in square brackets.\n",
    "#### This returns a Series object, the other fundamental data structure in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['Hours_Studied'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To just view the top 5 records of a particular column we can do as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hours_Studied'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Attendance', 'Parental_Involvement']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To view the unique values along with their counts in a particular column, we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Access_to_Resources'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving a row of the DataFrame using integer-based indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting attendance to a fraction of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attendance'] = df['Attendance']/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a new column is as simple as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['study_efficiency'] = df['Exam_Score'] / df['Hours_Studied']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "#### Deleting a row/column are shown in the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping a column\n",
    "df.drop('Tutoring_Sessions', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping a row (given an index)\n",
    "df.drop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering a dataframe based on a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_parental_involvement = df[df['Parental_Involvement'] == 'Low']\n",
    "high_scorers = df[df['Exam_Score'] > 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_parental_involvement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_scorers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to sort the dataframe in a particular order (ascending/descending) based on a particular column, we can do as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_exam_score = df.sort_values(by='Exam_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_exam_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using groupby, is a very powerful way of grouping 2 or more columns together and applying aggreagte functions to each group. It aids in the EDA process uncovering some really important insights pertaining to the dataset.\n",
    "#### It is widely used in data analysis to summarize, filter, and transform data by grouping similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average exam score by gender\n",
    "df.groupby('Gender')['Exam_Score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median average score by teacher quality\n",
    "df.groupby('Teacher_Quality')['Exam_Score'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can apply a transformation after grouping few columns and use the lambda function to perform some computation, in this case we standardize the values of Exam Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Family_Income')['Exam_Score'].transform(lambda x: (x-x.mean())/x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's plot some results!!\n",
    "#### We can use a combination of matplotlib and seaborn to plot some nice visualization of different statistical findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Matplotlib is a versatile and powerful Python library used for creating static, interactive, and animated visualizations. (line plots, histograms, scatter plots)\n",
    "- #### Seaborn is like Matplotlib but with an extra flair—pre-styled, easy-to-use visualizations like heatmaps, violin plots, and pair plots are its specialty. Seaborn takes care of aesthetics so you can focus on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Family_Income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Standardized_Scores'] = df.groupby('Family_Income')['Exam_Score'].transform(lambda x: (x-x.mean())/x.std())\n",
    "\n",
    "# Plot the standardized scores by Family Income\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Family_Income', y='Standardized_Scores', data=df)\n",
    "plt.title('Distribution of Standardized Exam Scores by Family Income')\n",
    "plt.xlabel('Family Income')\n",
    "plt.ylabel('Standardized Exam Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### The **IQR (Interquartile Range)** is the difference between the third quartile (Q3, the 75th percentile) and the first quartile (Q1, the 25th percentile). So, `IQR = Q3 - Q1`.\n",
    "- #### The whiskers represent the range of data points that are not considered outliers\n",
    "- #### The lower whisker extends to the smallest data point that is **at least** `Q1 - 1.5 * IQR`.\n",
    "- #### The upper whisker extends to the largest data point that is **at most** `Q3 + 1.5 * IQR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores_gender = df.groupby('Gender')['Exam_Score'].mean().reset_index()\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='Gender', y='Exam_Score', data=average_scores_gender)\n",
    "plt.title('Average Exam Scores by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Average Exam Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x='Hours_Studied', y='Exam_Score', data=df, hue='Gender', style='Gender', s=50)\n",
    "plt.title('Relationship Between Hours Studied and Exam Score')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.legend(title='Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Parental_Involvement', data=df, palette='autumn')\n",
    "plt.title('Count of Parental Involvement Levels')\n",
    "plt.xlabel('Parental Involvement')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto some Machine Learning!\n",
    "\n",
    "- #### Let's build a linear regression using sklearn\n",
    "\n",
    "- #### First we need to handle the categorical variables using One-Hot Encoding\n",
    "\n",
    "- #####  One-hot encoding is a technique used to convert categorical data into a numerical format by representing each category as a binary vector. \n",
    "- ##### Each unique category is transformed into a new column, and for a given row, only the column corresponding to the category is marked as 1, while the rest are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('study_efficiency', axis=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_variables = df.select_dtypes(include=['object']).columns.tolist()\n",
    "df_processed = pd.get_dummies(df, columns=cat_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We can do the same as above using OneHotEncoder of the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)  \n",
    "encoded_data = ohe.fit_transform(df[cat_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.DataFrame(encoded_data, columns=ohe.get_feature_names_out(cat_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the non categorical variable from the original to the encoded df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_encoded_df = pd.concat([df.drop(cat_variables, axis=1), encoded_df], axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_encoded_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this example, we're going to try and build a Exam Score predictor based on various features.\n",
    "- ##### Use all the columns except the Exam Score column as predictors (features) for the linear regression model.\n",
    "- ##### The Exam Score is the target variable, which we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X = complete_encoded_df.drop('Exam_Score', axis=1)\n",
    "y = complete_encoded_df['Exam_Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. \n",
    "- The goal is to find the line (or hyperplane in higher dimensions) that best fits the data points by minimizing the differences between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mean Squared Error (MSE)** is the average of the squared differences between the predicted values and the actual values. It is calculated using the formula:\n",
    "\n",
    "  $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "  where:\n",
    "  - `y_i`: actual value\n",
    "  - `ŷ_i`: predicted value\n",
    "  - `n`: number of data points\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)** is the square root of the MSE. It provides the error in the same units as the original data and is calculated using the formula:\n",
    "\n",
    "  $$\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "- **Mean Absolute Error (MAE)** is the average of the absolute differences between the predicted and actual values. It is calculated using the formula:\n",
    "\n",
    "  $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_preds)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_preds, squared=False)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge for you:\n",
    "#### The Mean Absolute Error is pretty good so far but, do you think we can improve it any further using any statistical techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df.drop(columns='Exam_Score')\n",
    "y = df['Exam_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Scale numerical columns\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = X_train.select_dtypes(\n",
    "    include=['int64', 'float64']).columns\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_columns] = scaler.fit_transform(\n",
    "    X_train[numerical_columns])\n",
    "X_test_scaled[numerical_columns] = scaler.transform(\n",
    "    X_test[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_correct = pd.get_dummies(\n",
    "X_train_scaled, columns=categorical_columns)\n",
    "X_test_correct = pd.get_dummies(X_test_scaled, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = LinearRegression()\n",
    "model_new.fit(X_train_correct, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model_new.predict(X_test_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
